{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "746220b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edc4564",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using device=mps ***\n",
      "\n",
      "Jupyter=True, MatPlotLib.isinteractive()=False\n",
      "Unique ID: WYyEvIWPSay5ysVt_bmvHQ\n",
      "PlotVideoMaker: Hyper-Training, auto-save=True\n",
      "middle-C=261.63 Hz\n",
      "PlotVideoMaker: STFT_Video, auto-save=True\n",
      "Using sample rate=44100 Hz, FFT=2048 buckets, hop=1024 samples, duration=2.0 sec = 86 time steps\n",
      "Max frequency=22050 Hz --> freq_buckets=1025\n",
      "STFT file already created: STFT 44100 Hz, size=1024, hop=1024.pkl\n",
      "1 sample = 1,025 x 86 = 88,150\n",
      "fail_loss=20000, last_saved_loss=10000\n",
      "\n",
      "\n",
      "\n",
      "Training model AudioConv_AE\n",
      "\n",
      "last_saved_loss= 5000\n",
      "Loaded 1017 samples from Audio 44100.pkl\n",
      "1017 samples\n",
      "Using train=813 samples, test=204 samples.\n",
      "train_best_params: AudioConv_AE: [4, -5, 4, 84, 168, 4]\n",
      "PlotVideoMaker: STFT - train AudioConv_AE, auto-save=True\n",
      "fail_loss=88200, last_saved_loss=44100\n",
      "AudioConv_AE layers=4, kernels=84, size=168, compression=4\n",
      "\tlayer 1: kernel=168, stride=42, length=2,097, compression=  42.1x\n",
      "\tlayer 2: kernel= 84, stride=21, length=   96, compression=  21.8x\n",
      "\tlayer 3: kernel= 42, stride=10, length=    6, compression=  16.0x\n",
      "\tlayer 4: kernel=  6, stride= 1, length=    1, compression=   6.0x\n",
      "set_freeze_depth None\n",
      "AduioConvAE: eval_depth=4 / 4 layers, enabled_layers=6, trainable 1,891,597 parameters\n",
      "\tencoded shape=(84, 1), size=84\n",
      "\tdecoded shape=(1, 83916), size=83916\n",
      "AudioConv_AE 1,891,597 parameters, compression=1050.0\n",
      "set_freeze_depth 2\n",
      "AduioConvAE: eval_depth=2 / 4 layers, enabled_layers=6, trainable 677,712 parameters\n",
      "AudioConv_AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 84, kernel_size=(168,), stride=(42,))\n",
      "    (1): Conv1d(84, 84, kernel_size=(84,), stride=(21,))\n",
      "    (2): Conv1d(84, 84, kernel_size=(42,), stride=(10,))\n",
      "    (3): Conv1d(84, 84, kernel_size=(6,), stride=(1,))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(84, 84, kernel_size=(6,), stride=(1,))\n",
      "    (1): ConvTranspose1d(84, 84, kernel_size=(42,), stride=(10,))\n",
      "    (2): ConvTranspose1d(84, 84, kernel_size=(84,), stride=(21,))\n",
      "    (3): ConvTranspose1d(84, 1, kernel_size=(168,), stride=(42,))\n",
      "  )\n",
      ")\n",
      "*** Inaccurate approximate size=1,891,597 vs actual size=677,712, error=179.12%\n",
      "model=AudioConv_AE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv1d(1, 84, kernel_size=(168,), stride=(42,))\n",
      "    (1): Conv1d(84, 84, kernel_size=(84,), stride=(21,))\n",
      "    (2): Conv1d(84, 84, kernel_size=(42,), stride=(10,))\n",
      "    (3): Conv1d(84, 84, kernel_size=(6,), stride=(1,))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose1d(84, 84, kernel_size=(6,), stride=(1,))\n",
      "    (1): ConvTranspose1d(84, 84, kernel_size=(42,), stride=(10,))\n",
      "    (2): ConvTranspose1d(84, 84, kernel_size=(84,), stride=(21,))\n",
      "    (3): ConvTranspose1d(84, 1, kernel_size=(168,), stride=(42,))\n",
      "  )\n",
      ")\n",
      "optimiser: Adam batch=16, learning_rate=0.00016, weight_decay=0\n",
      "model: AudioConv_AE layers=4, kernels=84, size=168, compression=4 (params=677,712, compression=1050.0x)\n",
      "train=813 samples, batch=16 --> 50.8 batches/epoch, device=mps\n",
      "Adam: 677,712 trainable parameters\n",
      "inputs = Torch.Tensor[16, 88200] x torch.float32, size=1,411,200 elements = 5,644,800 bytes, device=mps:0\n",
      "hiddens = Torch.Tensor[16, 84, 96] x torch.float32, size=129,024 elements = 516,096 bytes, device=mps:0\n",
      "decoded.x = Torch.Tensor[16, 1, 87444] x torch.float32, size=1,399,104 elements = 5,596,416 bytes, device=mps:0\n",
      "outputs = Torch.Tensor[16, 88200] x torch.float32, size=1,411,200 elements = 5,644,800 bytes, device=mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m set_fail_loss(\u001b[38;5;241m20_000\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#train_best_params(\"AudioConv_AE\", [4, -5, 5, 42, 78, 235], None) # 525x compression, loss=2.75, overfit=1.2\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_best_params(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudioConv_AE\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m168\u001b[39m, \u001b[38;5;241m4\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Coding/SampleGen/HyperParameterTuning.py:330\u001b[0m, in \u001b[0;36mtrain_best_params\u001b[0;34m(model_name, params, samples, finest)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m#max_time = hour\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m#set_display_hiddens(True) # Displays the internal auto-encoder output\u001b[39;00m\n\u001b[1;32m    329\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m train_model(model_name, params, max_epochs, max_time, max_params, max_overfit, max_loss, verbose, finest)\n",
      "File \u001b[0;32m~/Coding/SampleGen/Train.py:255\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, hyper_params, max_epochs, max_time, max_params, max_overfit, max_loss, verbose, load_existing)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 255\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    256\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Store the loss after each epoch:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from HyperParameterTuning import *\n",
    "\n",
    "\n",
    "#set_display_hiddens(True) # interesting but annoying\n",
    "\n",
    "#hypertrain_AudioConv_VAE()\n",
    "set_fail_loss(20_000)\n",
    "\n",
    "#train_best_params(\"AudioConv_AE\", [4, -5, 5, 42, 78, 235], None) # 525x compression, loss=2.75, overfit=1.2\n",
    "\n",
    "train_best_params(\"AudioConv_AE\", [4, -5, 4, 84, 168, 4], None)\n",
    "\n",
    "#train_best_params(\"AudioConv_AE\", [2, -4, 4, 40, 34, 30])\n",
    "#train_best_params(\"AudioConv_AE\", [3, -5, 6, 20, 100, 50])\n",
    "#train_best_params(\"AudioConv_AE\", [3, -5, 5, 33, 28, 28])\n",
    "\n",
    "#train_best_params(\"AudioConv_VAE_Incremental\", [2, -6, 9, 4, 1])\n",
    "#train_best_params(\"AudioConv_VAE_Incremental\", [2, -7, 12, 3, 10.0])\n",
    "\n",
    "#hypertrain_AudioConv_VAE()\n",
    "\n",
    "# from MakeSTFTs import *\n",
    "# from Train import *\n",
    "# from AudioUtils import *\n",
    "\n",
    "# Load a demo sample, convert to STFT and back and play the sound.\n",
    "#demo_stft(\"Samples/Piano C4 Major 13.wav\", 2048, 2048*3//4)\n",
    "#stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0542eb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read all available samples and convert to STFTs\n",
    "# This is performed automatically in the back-ground, but you can force it manually here with plots of the rejected audio files.\n",
    "#make_STFTs(True)\n",
    "\n",
    "#test_stft_conversions(\"Samples/Piano C4 Major 13 - 44.1 kHz.wav\")\n",
    "#stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff999e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the best hyper-parameters to train this model (typically needs to run overnight)\n",
    "#model = \"StepWiseMLP\" # or \"RNNAutoEncoder\" and RNN_VAE_Incremental\", or \"StepWiseMLP\" and \"MLPVAE_Incremental\"\n",
    "\n",
    "\n",
    "#model = \"RNNAutoEncoder\"\n",
    "#model = \"RNN_VAE\"\n",
    "#model = \"RNN_VAE_Incremental\"\n",
    "\n",
    "#model = \"StepWiseMLP\"\n",
    "#model = \"MLP_VAE\"\n",
    "#model = \"MLPVAE_Incremental\"\n",
    "\n",
    "#model = \"RNN_F&T\"\n",
    "\n",
    "#model = \"STFT_VAE\"\n",
    "\n",
    "#model = \"Conv2D_AE\"\n",
    "#model = \"Conv2D_VAE_Incremental\"\n",
    "\n",
    "model = \"AudioConv_AE\"\n",
    "#model = \"AudioConv_VAE_Incremental\"\n",
    "#model = \"AudioConv_VAE\"\n",
    "\n",
    "# First optimise the hyper-parameters for this model\n",
    "#optimise_hyper_parameters(model)\n",
    "\n",
    "# Then train using the best hyper-parameters\n",
    "#train_best_params(model)\n",
    "\n",
    "\n",
    "# Alternatively, refine the training for the best set of hyper-parameters we've found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a5095",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test and Generate new samples\n",
    "from Generate import *\n",
    "\n",
    "use_model(model)\n",
    "\n",
    "# Print some pretty pictures of the samples and their encodings\n",
    "#demo_encodings()\n",
    "\n",
    "# Test the accuracy of the model: lists all samples by decreasing accuracy\n",
    "test_all()\n",
    "\n",
    "# Generate samples, interpolate, morph etc.\n",
    "demo_sounds()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a2ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c20ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
